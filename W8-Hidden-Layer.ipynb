{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d84079f",
   "metadata": {},
   "source": [
    "# W8 - Hidden Layer\n",
    "\n",
    "This notebook contains personal notes and Python, Numpy and Latex of exercises from the \"AI by Hand ✍️ Workbook\" by Prof. Tom Yeh.\n",
    "\n",
    "\n",
    "Reference: [AI by Hand ✍️  Workbook](https://www.byhand.ai/t/workbook)\n",
    "\n",
    "*Contents*\n",
    "\n",
    "So far we covered one neuron layer. Now we are concatennating neuron layers. The output of one neuron is used as an output of the next one. \n",
    "\n",
    "Hidden neuron layers stil have a bias and activation component per layer. \n",
    "\n",
    "The architecture of this layers if flexible:\n",
    "\n",
    "- You can end th one vector $y$ per input vector $x$, useful for classification \n",
    "- Or just one $y$ per multiple $x$, useful for sensors and state predictions\n",
    "\n",
    "\n",
    "### Questions\n",
    "\n",
    "[Q] What is the official nomeclature of input vectors and output vectors?\n",
    "\n",
    "- $x$ is usually called features\n",
    "- $y$ is usually called targets or predictions\n",
    "- The output of a hidden layer is usually called *Activation Vector* $a$ or *Hidden State* $h$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1967565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9abc747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer result:\n",
      "[[4]]\n",
      "y:\n",
      "[[3]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1 \n",
    "x = np.matrix([[3,1]]) \n",
    "w = np.matrix([[1,1]])\n",
    "b = np.array([0])\n",
    "\n",
    "# hidden layer\n",
    "h = np.matrix([[1]]) # dim 1x1\n",
    "h_b = np.array([-1])\n",
    "\n",
    "a = ReLU(np.dot(w,x.T) + b)\n",
    "\n",
    "print(\"Hidden layer result:\")\n",
    "print(a)\n",
    "y = ReLU(np.dot(h, a.T) + h_b)\n",
    "\n",
    "print(\"y:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ad63e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer result:\n",
      "[[4]\n",
      " [3]\n",
      " [5]\n",
      " [1]]\n",
      "y:\n",
      "[[7]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 17 \n",
    "x = np.matrix([[2,1,3]]) \n",
    "w = np.matrix([\n",
    "    [0,1,1],\n",
    "    [1,1,0],\n",
    "    [1,0,1],\n",
    "    [1,0,0]\n",
    "    ]\n",
    "    )\n",
    "\n",
    "b = np.matrix([[0,0,0,-1]])\n",
    "\n",
    "# hidden layer\n",
    "h = np.matrix([\n",
    "    [1,1,0,0],\n",
    "    [0,0,1,1]])\n",
    "\n",
    "h_b = np.matrix([[0,-1]])\n",
    "\n",
    "a = ReLU(np.dot(w,x.T) + b.T) # bias is transposed here\n",
    "\n",
    "print(\"Hidden layer result:\")\n",
    "print(a)\n",
    "\n",
    "y = ReLU(np.dot(h, a) + h_b.T)\n",
    "\n",
    "print(\"y:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdc327ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer result:\n",
      "[[2]\n",
      " [3]\n",
      " [0]\n",
      " [2]\n",
      " [0]\n",
      " [1]]\n",
      "y:\n",
      "[[8]]\n"
     ]
    }
   ],
   "source": [
    "# Exercise 17 \n",
    "x = np.matrix([[3,2]]) \n",
    "w = np.matrix([\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [-1,1],\n",
    "    [0,0],\n",
    "    [0,-1],\n",
    "    [1,-1]\n",
    "    ]\n",
    "    )\n",
    "\n",
    "b = np.matrix([[0,0,0,2,0,0]])\n",
    "\n",
    "# hidden layer\n",
    "h = np.matrix([[1,1,1,1,1,1]])\n",
    "\n",
    "h_b = np.matrix([[0]])\n",
    "\n",
    "a = ReLU(np.dot(w,x.T) + b.T) # bias is transposed here\n",
    "\n",
    "print(\"Hidden layer result:\")\n",
    "print(a)\n",
    "\n",
    "y = ReLU(np.dot(h, a) + h_b.T)\n",
    "\n",
    "print(\"y:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c44c29",
   "metadata": {},
   "source": [
    "## Neural Network Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer: weights + biases + Activation Function\n",
    "# Concatenation or structure , list of layers\n",
    "# Input: np.matrix \n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Layer:\n",
    "    weights: np.matrix\n",
    "    bias: np.matrix\n",
    "    activation_f : Callable\n",
    "\n",
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, layers: list[Layer]):\n",
    "        self.layers = layers\n",
    "\n",
    "    def multiply(self, input):\n",
    "        '''\n",
    "        Processes the multiplications of Input and Layers in the neural network\n",
    "        input : X\n",
    "\n",
    "        '''\n",
    "        print(\"Input:\")\n",
    "        print(input)\n",
    "        results = input\n",
    "        for layer in self.layers:\n",
    "            print(\"Muiltiplying with weights:\")\n",
    "            print(layer.weights)\n",
    "            print(f\"Weights size: {layer.weights.shape}  ,  input size: {results.T.size}\")\n",
    "\n",
    "            results = layer.activation_f(np.dot(layer.weights, results.T) + layer.bias.T) \n",
    "            \n",
    "            # Transponsing it to work with the patter so far\n",
    "            results = results.T\n",
    "\n",
    "            print(results)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27c5dae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[3 2]]\n",
      "Muiltiplying with weights:\n",
      "[[ 0  1]\n",
      " [ 1  0]\n",
      " [-1  1]\n",
      " [ 0  0]\n",
      " [ 0 -1]\n",
      " [ 1 -1]]\n",
      "Weights size: (6, 2)  ,  input size: 2\n",
      "[[2 3 0 2 0 1]]\n",
      "Muiltiplying with weights:\n",
      "[[1 1 1 1 1 1]]\n",
      "Weights size: (1, 6)  ,  input size: 6\n",
      "[[8]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[8]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 17 using the NeuralNetwork\n",
    "\n",
    "layer1 = Layer(\n",
    "    weights= np.matrix([\n",
    "        [0,1],\n",
    "        [1,0],\n",
    "        [-1,1],\n",
    "        [0,0],\n",
    "        [0,-1],\n",
    "        [1,-1]]),\n",
    "    bias = np.matrix([[0,0,0,2,0,0]]),\n",
    "    activation_f= ReLU\n",
    ")\n",
    "\n",
    "layer2 = Layer(\n",
    "    weights= np.matrix([[1,1,1,1,1,1]]),\n",
    "    bias= np.matrix([[0]]),\n",
    "    activation_f=ReLU\n",
    "    )\n",
    "    \n",
    "\n",
    "nn = NeuralNetwork(layers=[layer1, layer2])\n",
    "nn.multiply(input=np.matrix([[3,2]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibyhand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
